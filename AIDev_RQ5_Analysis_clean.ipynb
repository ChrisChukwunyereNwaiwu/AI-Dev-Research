{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisChukwunyereNwaiwu/AI-Dev-Research/blob/main/AIDev_RQ5_Analysis_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkBZMVe9Nyx"
      },
      "source": [
        "Imports & config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1xqoRvBs9Fes"
      },
      "outputs": [],
      "source": [
        "# ---- ALL LIBRARIES AT THE TOP ----\n",
        "import os, re, json, math, warnings, random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import os, math, json, re, numpy as np, pandas as pd\n",
        "from textwrap import fill\n",
        "import datetime as dt\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_colwidth\", 180)\n",
        "Path(\"results\").mkdir(exist_ok=True)\n",
        "\n",
        "RNG_SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB6SZla3CEpe"
      },
      "outputs": [],
      "source": [
        "!pip -q install python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy6vunC-9UBb"
      },
      "source": [
        "Load AIDev (PR-only is fine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe12M3uS9ZpV"
      },
      "outputs": [],
      "source": [
        "ds_pr = load_dataset(\"hao-li/AIDev\", \"all_pull_request\")\n",
        "pr_df = ds_pr[\"train\"].to_pandas()\n",
        "print(\"PRs:\", len(pr_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV5QMMkE9e1P"
      },
      "source": [
        "**Core feature engineering (shared by 5a/5b/5c)**\n",
        "\n",
        "\n",
        "Build: accepted, clarify_strict (and clarify_var), log_stars, pr_size_log1p, changed_files_log1p, language, touched paths + text patterns (for 5a), security flags (for 5c), and timestamps (optional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ9Riws-9u1P"
      },
      "outputs": [],
      "source": [
        "# --- helpers (accepted, clarify, paths) ---\n",
        "QWORDS = re.compile(r\"\\b(what|when|where|why|how|should|could|would|which|if)\\b\", re.I)\n",
        "code_block  = re.compile(r\"```.+?```\", re.S)\n",
        "inline_code = re.compile(r\"`[^`]+`\")\n",
        "url_pat     = re.compile(r\"https?://\\S+\")\n",
        "\n",
        "def robust_accepted(df):\n",
        "    acc = pd.Series(False, index=df.index)\n",
        "    for col in [\"merged\",\"is_merged\",\"merge\",\"merged_bool\"]:\n",
        "        if col in df.columns:\n",
        "            acc |= df[col].astype(str).str.strip().str.lower().isin([\"true\",\"1\",\"yes\",\"y\"])\n",
        "    for col in [\"merged_at\",\"mergedAt\",\"merge_at\",\"merge_date\"]:\n",
        "        if col in df.columns:\n",
        "            acc |= (df[col].astype(str).str.strip().ne(\"\") & df[col].notna())\n",
        "    for col in [\"state\",\"status\",\"merge_state\"]:\n",
        "        if col in df.columns:\n",
        "            s = df[col].astype(str).str.strip().str.lower()\n",
        "            acc |= s.eq(\"merged\")\n",
        "    return acc.astype(int)\n",
        "\n",
        "def clean_text(x:str)->str:\n",
        "    if not isinstance(x,str): return \"\"\n",
        "    x = code_block.sub(\" \", x); x = inline_code.sub(\" \", x); x = url_pat.sub(\" \", x)\n",
        "    return x\n",
        "\n",
        "def clarify_strict_row(t,b):\n",
        "    t,b = clean_text(t), clean_text(b)\n",
        "    return int((\"?\" in t or \"?\" in b) and (bool(QWORDS.search(t)) or bool(QWORDS.search(b))))\n",
        "\n",
        "def parse_paths_cell(x):\n",
        "    if x is None or (isinstance(x,float) and math.isnan(x)): return []\n",
        "    if isinstance(x, list): return [str(y) for y in x]\n",
        "    s = str(x).strip()\n",
        "    if not s: return []\n",
        "    if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"{\") and s.endswith(\"}\")):\n",
        "        try:\n",
        "            obj = json.loads(s)\n",
        "            if isinstance(obj, list): return [str(y) for y in obj]\n",
        "            if isinstance(obj, dict) and isinstance(obj.get(\"files\"), list):\n",
        "                return [str(y) for y in obj[\"files\"]]\n",
        "        except: pass\n",
        "    if any(d in s for d in [\",\",\";\",\"\\n\"]):\n",
        "        return [p.strip() for p in re.split(r\"[,\\n;]+\", s) if p.strip()]\n",
        "    return [s]\n",
        "\n",
        "def extract_paths_series(df):\n",
        "    for col in [\"changed_files_list\",\"files_changed_list\",\"file_paths\",\"paths\",\"filenames\",\"files\",\"changed_files_details\"]:\n",
        "        if col in df.columns:\n",
        "            return df[col].apply(parse_paths_cell)\n",
        "    return pd.Series([[]]*len(df), index=df.index)\n",
        "\n",
        "# --- build df ---\n",
        "df = pr_df.copy()\n",
        "df[\"title\"] = df.get(\"title\", pd.Series(\"\", index=df.index)).fillna(\"\").astype(str)\n",
        "df[\"body\"]  = df.get(\"body\", pd.Series(\"\", index=df.index)).fillna(\"\").astype(str)\n",
        "df[\"accepted\"] = robust_accepted(df)\n",
        "\n",
        "# controls\n",
        "df[\"stars\"] = pd.to_numeric(df.get(\"stars\", df.get(\"stargazers_count\", df.get(\"watchers\", pd.Series(0, index=df.index)))), errors=\"coerce\").fillna(0)\n",
        "df[\"log_stars\"] = np.log1p(df[\"stars\"])\n",
        "df[\"additions\"] = pd.to_numeric(df.get(\"additions\", pd.Series(0, index=df.index)), errors=\"coerce\").fillna(0)\n",
        "df[\"deletions\"] = pd.to_numeric(df.get(\"deletions\", pd.Series(0, index=df.index)), errors=\"coerce\").fillna(0)\n",
        "df[\"pr_size_log1p\"] = np.log1p(df[\"additions\"] + df[\"deletions\"])\n",
        "df[\"changed_files_n\"] = pd.to_numeric(df.get(\"changed_files\", pd.Series(0, index=df.index)), errors=\"coerce\").fillna(0)\n",
        "df[\"changed_files_log1p\"] = np.log1p(df[\"changed_files_n\"])\n",
        "df[\"language\"] = df.get(\"language\", pd.Series(\"Unknown\", index=df.index)).fillna(\"Unknown\").astype(str)\n",
        "\n",
        "# clarify vars\n",
        "df[\"clarify_strict\"] = [clarify_strict_row(t,b) for t,b in zip(df[\"title\"], df[\"body\"])]\n",
        "clarify_var = \"clarify_strict\" if df[\"clarify_strict\"].nunique()==2 else \"clarify\"\n",
        "\n",
        "# touched paths + text patterns (5a)\n",
        "paths_series = extract_paths_series(df)\n",
        "def flag_paths(paths, rx): return int(any(re.search(rx, p, flags=re.I) for p in paths))\n",
        "df[\"touch_tests\"] = paths_series.apply(lambda ps: flag_paths(ps, r\"(^|/)(test|tests|spec|__tests__)(/|$)\"))\n",
        "df[\"touch_docs\"]  = paths_series.apply(lambda ps: flag_paths(ps, r\"(^|/)(docs?|doc|readme\\.md)(/|$)|\\.md$\"))\n",
        "df[\"touch_src\"]   = paths_series.apply(lambda ps: flag_paths(ps, r\"(^|/)(src|lib)(/|$)\"))\n",
        "df[\"touch_deps\"]  = paths_series.apply(lambda ps: flag_paths(ps, r\"(package(-lock)?\\.json|yarn\\.lock|pnpm-lock\\.yaml|requirements\\.txt|poetry\\.lock|setup\\.py|pyproject\\.toml|pom\\.xml|go\\.mod|Cargo\\.toml|build\\.gradle(\\.kts)?)\"))\n",
        "df[\"docs_only\"]   = ((df[\"touch_docs\"]==1) & (df[\"touch_src\"]==0) & (df[\"touch_tests\"]==0)).astype(int)\n",
        "\n",
        "PATTERNS = {\n",
        "    \"pat_docs\": r\"\\b(doc(s)?|readme)\\b\",\n",
        "    \"pat_build_ci\": r\"\\b(ci|build|pipeline|workflow|github actions)\\b\",\n",
        "    \"pat_lint\": r\"\\b(lint|format|prettier|flake8|eslint|black)\\b\",\n",
        "    \"pat_revert_hotfix\": r\"\\b(revert|rollback|hotfix)\\b\",\n",
        "    \"pat_test_fail\": r\"\\b(test(s)?\\s*(is|are)?\\s*failing|fail(ed|ure)|flake)\\b\",\n",
        "}\n",
        "for name, rx in PATTERNS.items():\n",
        "    df[name] = (df[\"title\"].str.contains(rx, case=False, regex=True, na=False) |\n",
        "                df[\"body\"].str.contains(rx, case=False, regex=True, na=False)).astype(int)\n",
        "\n",
        "# security (5c)\n",
        "SEC_RX    = r\"\\b(security|vulnerab|cve-|cwe-|xss|csrf|rce|injection|auth|encrypt|token|credential)\\b\"\n",
        "BUMP_RX   = r\"\\b(bump|upgrade|update)\\b\"\n",
        "SECRET_RX = r\"\\b(password|apikey|api[-_ ]?key|secret|token|access[-_ ]?key|private[-_ ]?key)\\b\"\n",
        "df[\"sec_text\"]         = (df[\"title\"].str.contains(SEC_RX, case=False, regex=True, na=False) |\n",
        "                          df[\"body\"].str.contains(SEC_RX, case=False, regex=True, na=False)).astype(int)\n",
        "df[\"sec_dep_bump\"]     = (df[\"body\"].str.contains(BUMP_RX, case=False, regex=True, na=False) & (df[\"touch_deps\"]==1)).astype(int)\n",
        "df[\"sec_secret_terms\"] = (df[\"title\"].str.contains(SECRET_RX, case=False, regex=True, na=False) |\n",
        "                          df[\"body\"].str.contains(SECRET_RX, case=False, regex=True, na=False)).astype(int)\n",
        "df[\"security_flag\"]    = ((df[\"sec_text\"]==1) | (df[\"sec_dep_bump\"]==1) | (df[\"sec_secret_terms\"]==1)).astype(int)\n",
        "\n",
        "print(\"Engineered columns ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwZ1_rH7-hpu"
      },
      "source": [
        "Utilities (bootstrap + small helpers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX_vyb77-jZA"
      },
      "outputs": [],
      "source": [
        "def bootstrap_accept_gap(d, col, n_boot=1000, seed=RNG_SEED):\n",
        "    d = d[[col,\"accepted\"]].dropna()\n",
        "    d0 = d.loc[d[col]==0, \"accepted\"].to_numpy()\n",
        "    d1 = d.loc[d[col]==1, \"accepted\"].to_numpy()\n",
        "    if len(d0)==0 or len(d1)==0:\n",
        "        return np.nan, (np.nan, np.nan), len(d0), len(d1)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    gaps = [rng.choice(d1, len(d1), True).mean() - rng.choice(d0, len(d0), True).mean()\n",
        "            for _ in range(n_boot)]\n",
        "    lo, hi = np.percentile(gaps, [2.5, 97.5])\n",
        "    return float(np.mean(gaps)), (float(lo), float(hi)), len(d0), len(d1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFnzbvyT-muk"
      },
      "source": [
        "Section A/B/C cells\n",
        "\n",
        " RQ 5(a): Failure patterns & touched paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtcSdttt-uR4"
      },
      "outputs": [],
      "source": [
        "print(\"===== RQ 5(a): Failure patterns & touched paths =====\")\n",
        "fail_cols = [c for c in df.columns if c.startswith(\"pat_\")]\n",
        "path_cols = [\"docs_only\",\"touch_tests\",\"touch_docs\",\"touch_deps\",\"touch_src\"]\n",
        "\n",
        "# Prevalence %\n",
        "prev_5a = (pd.concat([df[fail_cols].mean(), df[path_cols].mean()])*100).sort_values(ascending=False).round(2)\n",
        "print(\"Top prevalence signals:\\n\", prev_5a.head(10))\n",
        "\n",
        "# Δ acceptance (pp)\n",
        "def acc_delta(col):\n",
        "    g = df.groupby(col)[\"accepted\"].mean()\n",
        "    return float((g.get(1,np.nan) - g.get(0,np.nan))*100)\n",
        "deltas_5a = pd.Series({c: acc_delta(c) for c in (fail_cols+path_cols)}).sort_values(ascending=False)\n",
        "print(\"\\nTop Δ acceptance signals (pp):\\n\", deltas_5a.head(10))\n",
        "\n",
        "# Small per-signal logit (top 3 by prevalence)\n",
        "top3 = [c for c in prev_5a.index if df[c].nunique()==2][:3]\n",
        "for s in top3:\n",
        "    keep = [\"accepted\", s, clarify_var, \"language\", \"log_stars\", \"pr_size_log1p\"]\n",
        "    d = df[keep].dropna().copy()\n",
        "    if d[\"accepted\"].nunique()==2 and d[s].nunique()==2:\n",
        "        try:\n",
        "            m = smf.logit(f\"accepted ~ {s} + {clarify_var} + log_stars + pr_size_log1p + C(language)\", data=d).fit(disp=False)\n",
        "            print(f\"\\n[5(a)] Logit with {s}:\\n\", m.summary().tables[1])\n",
        "        except Exception as e:\n",
        "            print(f\"[5(a)] logit with {s} failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2EpcYRm-3vh"
      },
      "source": [
        "RQ 5(b): Early signals (clarify) → acceptance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuwu5w9k-4lj"
      },
      "outputs": [],
      "source": [
        "print(\"===== RQ 5(b): Early signals → acceptance =====\")\n",
        "acc_overall = df[\"accepted\"].mean()\n",
        "acc0 = df.loc[df[clarify_var]==0, \"accepted\"].mean() if (df[clarify_var]==0).any() else np.nan\n",
        "acc1 = df.loc[df[clarify_var]==1, \"accepted\"].mean() if (df[clarify_var]==1).any() else np.nan\n",
        "gap_mean, (gap_lo, gap_hi), n0, n1 = bootstrap_accept_gap(df, clarify_var, n_boot=1000)\n",
        "\n",
        "print(f\"N={len(df):,} | overall acc={acc_overall*100:.2f}% | {clarify_var}=0: {acc0*100:.2f}% | {clarify_var}=1: {acc1*100:.2f}%\")\n",
        "print(f\"Δ acc (pp)={(acc1-acc0)*100:.2f} | 95% CI [{gap_lo*100:.2f}, {gap_hi*100:.2f}]\")\n",
        "\n",
        "# Regularized LR with controls\n",
        "keep = [\"accepted\", clarify_var, \"language\", \"log_stars\", \"pr_size_log1p\"]\n",
        "d = df[keep].dropna().copy()\n",
        "if d[\"accepted\"].nunique()==2:\n",
        "    X = d[[clarify_var,\"language\",\"log_stars\",\"pr_size_log1p\"]]; y = d[\"accepted\"]\n",
        "    pre = ColumnTransformer([(\"lang\", OneHotEncoder(handle_unknown=\"ignore\"), [\"language\"])], remainder=\"passthrough\")\n",
        "    clf = Pipeline([(\"pre\", pre), (\"lr\", LogisticRegression(max_iter=700, solver=\"lbfgs\", class_weight=\"balanced\"))])\n",
        "    clf.fit(X,y)\n",
        "    X1, X0 = X.copy(), X.copy()\n",
        "    X1[clarify_var] = 1; X0[clarify_var] = 0\n",
        "    me_c = clf.predict_proba(X1)[:,1].mean() - clf.predict_proba(X0)[:,1].mean()\n",
        "    auc  = roc_auc_score(y, clf.predict_proba(X)[:,1])\n",
        "    print(f\"Regularized Δprob={me_c*100:.2f} pp | AUC={auc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dbvNOmC_AnO"
      },
      "source": [
        "RQ 5(c): Security signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pELAbfml_FH-"
      },
      "outputs": [],
      "source": [
        "print(\"===== RQ 5(c): Security signals =====\")\n",
        "sec_prev = df[\"security_flag\"].mean()*100 if \"security_flag\" in df.columns else np.nan\n",
        "print(f\"security_flag prevalence: {sec_prev:.2f}%\")\n",
        "\n",
        "if \"security_flag\" in df.columns and df[\"security_flag\"].nunique()==2:\n",
        "    sec_acc = df.groupby(\"security_flag\")[\"accepted\"].mean()*100\n",
        "    print(\"Acceptance by security_flag:\\n\", sec_acc.rename(index={0:\"No\",1:\"Yes\"}))\n",
        "\n",
        "    # Optional time-to-merge (merged only, if timestamps exist)\n",
        "    for tcol in [\"created_at\",\"merged_at\",\"closed_at\",\"updated_at\"]:\n",
        "        if tcol in df.columns:\n",
        "            df[tcol] = pd.to_datetime(df[tcol], errors=\"coerce\", utc=True)\n",
        "    if {\"created_at\",\"merged_at\"}.issubset(df.columns):\n",
        "        d_m = df[df[\"accepted\"]==1].copy()\n",
        "        ttm = (d_m[\"merged_at\"] - d_m[\"created_at\"]).dt.total_seconds()/(3600*24)\n",
        "        if ttm.notna().any():\n",
        "            med0 = ttm[d_m[\"security_flag\"]==0].median()\n",
        "            med1 = ttm[d_m[\"security_flag\"]==1].median()\n",
        "            print(f\"Median TTM (days): security={med1:.2f} vs non-security={med0:.2f}\")\n",
        "\n",
        "    # Focused logit\n",
        "    keep = [\"accepted\",\"security_flag\",clarify_var,\"language\",\"log_stars\",\"pr_size_log1p\"]\n",
        "    d = df[keep].dropna().copy()\n",
        "    if d[\"accepted\"].nunique()==2:\n",
        "        try:\n",
        "            m_sec = smf.logit(f\"accepted ~ security_flag + {clarify_var} + log_stars + pr_size_log1p + C(language)\", data=d).fit(disp=False)\n",
        "            print(\"\\n[5(c)] security_flag model:\\n\", m_sec.summary().tables[1])\n",
        "        except Exception as e:\n",
        "            print(\"security_flag logit failed:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWtKu40ABFER"
      },
      "source": [
        "compile RQ 5(a), 5(b), 5(c) into one page + optional DOCX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jhTLeik_3o3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# ---------- small helpers ----------\n",
        "def fmt_pct(x, digits=2):\n",
        "    return \"n/a\" if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))) else f\"{x*100:.{digits}f}%\"\n",
        "\n",
        "def fmt_pp(x, digits=2):\n",
        "    return \"n/a\" if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))) else f\"{x*100:.{digits}f}\"\n",
        "\n",
        "def fmt_num(x):\n",
        "    try: return f\"{int(x):,}\"\n",
        "    except: return \"n/a\"\n",
        "\n",
        "def bootstrap_accept_gap(d, col, n_boot=1000, seed=42):\n",
        "    d = d[[col, \"accepted\"]].dropna()\n",
        "    d0 = d.loc[d[col]==0, \"accepted\"].to_numpy()\n",
        "    d1 = d.loc[d[col]==1, \"accepted\"].to_numpy()\n",
        "    if len(d0)==0 or len(d1)==0:\n",
        "        return np.nan, (np.nan, np.nan), len(d0), len(d1)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    gaps = []\n",
        "    for _ in range(n_boot):\n",
        "        gaps.append(rng.choice(d1, len(d1), True).mean() - rng.choice(d0, len(d0), True).mean())\n",
        "    gaps = np.array(gaps)\n",
        "    lo, hi = np.percentile(gaps, [2.5, 97.5])\n",
        "    return float(gaps.mean()), (float(lo), float(hi)), len(d0), len(d1)\n",
        "\n",
        "# ---------- shared objects from earlier cells (rebuild if missing) ----------\n",
        "assert \"df\" in globals(), \"I need the main dataframe `df` from previous cells.\"\n",
        "\n",
        "# Clarify column used throughout\n",
        "clarify_col = \"clarify_strict\" if (\"clarify_strict\" in df.columns and df[\"clarify_strict\"].nunique()==2) \\\n",
        "              else (\"clarify\" if \"clarify\" in df.columns else None)\n",
        "\n",
        "# Controls (recompute if not present)\n",
        "if \"log_stars\" not in df.columns:\n",
        "    df[\"stars\"] = pd.to_numeric(df.get(\"stars\", df.get(\"stargazers_count\", df.get(\"watchers\", 0))), errors=\"coerce\").fillna(0)\n",
        "    df[\"log_stars\"] = np.log1p(df[\"stars\"])\n",
        "if \"pr_size_log1p\" not in df.columns:\n",
        "    df[\"additions\"] = pd.to_numeric(df.get(\"additions\", 0), errors=\"coerce\").fillna(0)\n",
        "    df[\"deletions\"] = pd.to_numeric(df.get(\"deletions\", 0), errors=\"coerce\").fillna(0)\n",
        "    df[\"pr_size_log1p\"] = np.log1p(df[\"additions\"] + df[\"deletions\"])\n",
        "\n",
        "# ---------- 5(b): Early signals → acceptance ----------\n",
        "acc_overall = float(df[\"accepted\"].mean()) if df[\"accepted\"].nunique()>0 else np.nan\n",
        "acc0 = float(df.loc[df[clarify_col]==0, \"accepted\"].mean()) if clarify_col and (df[clarify_col]==0).any() else np.nan\n",
        "acc1 = float(df.loc[df[clarify_col]==1, \"accepted\"].mean()) if clarify_col and (df[clarify_col]==1).any() else np.nan\n",
        "gap_mean, (gap_lo, gap_hi), n0, n1 = (np.nan, (np.nan, np.nan), 0, 0)\n",
        "if clarify_col:\n",
        "    gap_mean, (gap_lo, gap_hi), n0, n1 = bootstrap_accept_gap(df, clarify_col, n_boot=1000)\n",
        "\n",
        "# Try to fetch Option-C Δprob & AUC from earlier, else compute quickly\n",
        "me_c_val = None; auc_val = None\n",
        "try:\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    from sklearn.compose import ColumnTransformer\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    keep = [\"accepted\", clarify_col, \"language\", \"log_stars\", \"pr_size_log1p\"]\n",
        "    if clarify_col and all(c in df.columns for c in keep):\n",
        "        dC = df[keep].dropna().copy()\n",
        "        if dC[\"accepted\"].nunique()==2:\n",
        "            X = dC[[clarify_col,\"language\",\"log_stars\",\"pr_size_log1p\"]]\n",
        "            y = dC[\"accepted\"]\n",
        "            pre = ColumnTransformer([(\"lang\", OneHotEncoder(handle_unknown=\"ignore\"), [\"language\"])],\n",
        "                                    remainder=\"passthrough\")\n",
        "            clf = Pipeline([(\"pre\", pre),\n",
        "                            (\"lr\", LogisticRegression(max_iter=700, solver=\"lbfgs\", class_weight=\"balanced\"))])\n",
        "            clf.fit(X,y)\n",
        "            X1, X0 = X.copy(), X.copy()\n",
        "            X1[clarify_col] = 1; X0[clarify_col] = 0\n",
        "            me_c_val = float(clf.predict_proba(X1)[:,1].mean() - clf.predict_proba(X0)[:,1].mean())\n",
        "            auc_val  = float(roc_auc_score(y, clf.predict_proba(X)[:,1]))\n",
        "except Exception as e:\n",
        "    print(\"Option-C Δprob/AUC skipped:\", e)\n",
        "\n",
        "# ---------- 5(a): Failure patterns & touched paths ----------\n",
        "fail_cols = [c for c in df.columns if c.startswith(\"pat_\")]\n",
        "path_cols = [c for c in [\"docs_only\",\"touch_tests\",\"touch_docs\",\"touch_deps\",\"touch_src\"] if c in df.columns]\n",
        "\n",
        "prev_5a = (pd.concat([df[fail_cols].mean(), df[path_cols].mean()])*100).sort_values(ascending=False).round(2) \\\n",
        "          if (fail_cols or path_cols) else pd.Series(dtype=float)\n",
        "\n",
        "def acc_delta(col):\n",
        "    g = df.groupby(col)[\"accepted\"].mean()\n",
        "    return float((g.get(1,np.nan) - g.get(0,np.nan))*100)\n",
        "\n",
        "deltas_5a = pd.Series({c: acc_delta(c) for c in (fail_cols+path_cols)}).sort_values(ascending=False) \\\n",
        "            if (fail_cols or path_cols) else pd.Series(dtype=float)\n",
        "\n",
        "top_prev_items = prev_5a.head(3).to_dict() if len(prev_5a) else {}\n",
        "top_delta_items = deltas_5a.dropna().abs().sort_values(ascending=False).head(3)\n",
        "top_delta_items = top_delta_items.index.map(lambda k: (k, deltas_5a[k])).tolist() if len(deltas_5a) else []\n",
        "\n",
        "# ---------- 5(c): Security ----------\n",
        "sec_prev = float(df[\"security_flag\"].mean()*100) if \"security_flag\" in df.columns else np.nan\n",
        "sec_acc_no = sec_acc_yes = np.nan\n",
        "if \"security_flag\" in df.columns and df[\"security_flag\"].nunique()==2:\n",
        "    grp = df.groupby(\"security_flag\")[\"accepted\"].mean()*100\n",
        "    sec_acc_no, sec_acc_yes = float(grp.get(0,np.nan)), float(grp.get(1,np.nan))\n",
        "\n",
        "# time-to-merge medians (merged only), if timestamps exist\n",
        "ttm_line = \"\"\n",
        "if {\"created_at\",\"merged_at\"}.issubset(df.columns):\n",
        "    d_merged = df[df[\"accepted\"]==1].copy()\n",
        "    d_merged[\"created_at\"] = pd.to_datetime(d_merged[\"created_at\"], errors=\"coerce\", utc=True)\n",
        "    d_merged[\"merged_at\"]  = pd.to_datetime(d_merged[\"merged_at\"],  errors=\"coerce\", utc=True)\n",
        "    ttm = (d_merged[\"merged_at\"] - d_merged[\"created_at\"]).dt.total_seconds()/(3600*24)\n",
        "    if \"security_flag\" in d_merged.columns and d_merged[\"security_flag\"].nunique()==2 and ttm.notna().any():\n",
        "        med0 = float(ttm[d_merged[\"security_flag\"]==0].median())\n",
        "        med1 = float(ttm[d_merged[\"security_flag\"]==1].median())\n",
        "        if not (np.isnan(med0) or np.isnan(med1)):\n",
        "            ttm_line = f\" Median time-to-merge among merged PRs is {med1:.2f} vs {med0:.2f} days (security vs. non-security).\"\n",
        "\n",
        "# ---------- compose text ----------\n",
        "N = fmt_num(len(df))\n",
        "p_acc_overall = fmt_pct(acc_overall)\n",
        "p_acc0, p_acc1 = fmt_pct(acc0), fmt_pct(acc1)\n",
        "p_gap = fmt_pp(acc1-acc0) if (not np.isnan(acc1) and not np.isnan(acc0)) else \"n/a\"\n",
        "p_gap_lo, p_gap_hi = fmt_pp(gap_lo), fmt_pp(gap_hi)\n",
        "p_me_c = fmt_pp(me_c_val) if me_c_val is not None else \"n/a\"\n",
        "p_auc  = f\"{auc_val:.3f}\" if (auc_val is not None and not np.isnan(auc_val)) else \"n/a\"\n",
        "\n",
        "h = \"# Results for AIDev RQ 5(a), 5(b), 5(c)\\n\\n\"\n",
        "stamp = f\"_Generated {dt.datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}_\\n\\n\"\n",
        "\n",
        "para_5b = (\n",
        "    f\"**RQ 5(b) — Early signals (clarification) and acceptance.** \"\n",
        "    f\"Using the early textual cue **`{clarify_col}`**, we analyze **N={N}** agentic PRs. \"\n",
        "    f\"Overall acceptance is **{p_acc_overall}**. PRs without the cue are accepted at **{p_acc0}**, \"\n",
        "    f\"versus **{p_acc1}** with the cue — a raw difference of **{p_gap} pp** \"\n",
        "    f\"(bootstrap 95% CI **[{p_gap_lo}, {p_gap_hi}]**). A regularized logistic model controlling for \"\n",
        "    f\"repository popularity and PR size estimates an average change of **{p_me_c} pp** in acceptance \"\n",
        "    f\"probability when toggling the cue (**AUC={p_auc}**).\"\n",
        ")\n",
        "\n",
        "if top_prev_items:\n",
        "    prev_bits = \", \".join([f\"`{k}` ({v:.1f}%)\" for k,v in top_prev_items.items()])\n",
        "else:\n",
        "    prev_bits = \"no high-prevalence patterns in this slice\"\n",
        "\n",
        "if top_delta_items:\n",
        "    delta_bits = \", \".join([f\"`{k}`: {val:+.2f} pp\" for k,val in top_delta_items])\n",
        "else:\n",
        "    delta_bits = \"no patterns showed a stable acceptance difference\"\n",
        "\n",
        "para_5a = (\n",
        "    f\"**RQ 5(a) — Failure patterns and touched paths.** \"\n",
        "    f\"We instrument failure/path signals from PR text and touched files. \"\n",
        "    f\"The most prevalent signals are {prev_bits}. Acceptance differences by signal indicate {delta_bits}. \"\n",
        "    f\"These associations highlight where agentic PRs most often struggle.\"\n",
        ")\n",
        "\n",
        "p_sec_prev   = \"n/a\" if np.isnan(sec_prev) else f\"{sec_prev:.2f}%\"\n",
        "p_sec_acc_no = \"n/a\" if np.isnan(sec_acc_no) else f\"{sec_acc_no:.2f}%\"\n",
        "p_sec_acc_yes= \"n/a\" if np.isnan(sec_acc_yes) else f\"{sec_acc_yes:.2f}%\"\n",
        "para_5c = (\n",
        "    f\"**RQ 5(c) — Security-related signals.** \"\n",
        "    f\"Using conservative textual and dependency-bump heuristics, **{p_sec_prev}** of PRs are security-flagged. \"\n",
        "    f\"Their acceptance rate is **{p_sec_acc_yes}** vs **{p_sec_acc_no}** for non-flagged PRs.\"\n",
        "    f\"{ttm_line}\"\n",
        ")\n",
        "\n",
        "md_text = h + stamp + \"\\n\\n\".join([fill(para_5b, 110), fill(para_5a, 110), fill(para_5c, 110)]) + \"\\n\"\n",
        "with open(\"results/results_5abc.md\",\"w\") as f:\n",
        "    f.write(md_text)\n",
        "print(md_text)\n",
        "print(\"Saved:\", \"results/results_5abc.md\")\n",
        "\n",
        "# ---------- optional DOCX with plots (safe no-ops if lib/images missing) ----------\n",
        "try:\n",
        "    # !pip -q install python-docx\n",
        "    from docx import Document\n",
        "    from docx.shared import Inches\n",
        "    doc = Document()\n",
        "    doc.add_heading('AIDev — Results for RQ 5(a), 5(b), 5(c)', 0)\n",
        "    for para in [para_5b, para_5a, para_5c]:\n",
        "        doc.add_paragraph(para)\n",
        "\n",
        "    # attach any figures you already saved\n",
        "    figs = [\n",
        "        \"results/acceptance_rate_bar.png\",\n",
        "        \"results/acceptance_by_language.png\",\n",
        "        \"results/acceptance_by_quartile.png\",\n",
        "        \"results/5a_prevalence_top10.png\",\n",
        "        \"results/5a_acc_delta_top10.png\",\n",
        "        \"results/5c_acceptance_bar.png\",\n",
        "    ]\n",
        "    for img in figs:\n",
        "        if os.path.exists(img):\n",
        "            doc.add_picture(img, width=Inches(5.8))\n",
        "    out_docx = \"AIDev_5abc_Summary.docx\"\n",
        "    doc.save(out_docx)\n",
        "    print(\"Saved:\", out_docx)\n",
        "except Exception as e:\n",
        "    print(\"DOCX step skipped (install python-docx if needed).\", e)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}